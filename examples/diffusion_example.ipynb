{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3ee251-c456-4048-8fed-bd0057602c6f",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><span style=\"color:red\">This example that can be overwritten by container updates.</span></h3>\n",
    "<h4 style=\"text-align: center;\"><span style=\"color:red\">Please create a new notebook if you plan to make changes.</span></h4>\n",
    "\n",
    "## Intro\n",
    "\n",
    "This example jupyter file demonstrates basic [stable diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_2#texttoimage) functionality.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Run each cell to step through the process, including (first-time only) downloading a model to use.\n",
    "Note: Run cells, in order, at least once, to set up the environment.  Once set up, you can change the prompt and rerun just that cell.  If you make changes in a cell, that cell should be rerun for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f81453-3be5-4420-8500-8042fbd3480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_device, device_info\n",
    "\n",
    "device = get_device()\n",
    "device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57c8c6-5fc1-495a-89f2-629c8587d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    name: str\n",
    "    repo_id: str\n",
    "    variant: str = None\n",
    "    torch_dtype: str = None\n",
    "    foldername: str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Set the value of foldername based on repo_id\n",
    "        self.foldername = self.repo_id.split(\"/\")[1]\n",
    "\n",
    "    def fp16(self):\n",
    "        self.variant = \"fp16\"\n",
    "        self.torch_dtype = torch.float16\n",
    "        return self\n",
    "\n",
    "    def pretrain_options(self):\n",
    "        return self._filter([\"name\", \"repo_id\", \"foldername\"])\n",
    "\n",
    "    def _filter(self, excludes):\n",
    "        # Return the members that aren't set to None or in excludes\n",
    "        return {k:v for k,v in asdict(self).items() if (v is not None and k not in excludes)}\n",
    "\n",
    "models = {\n",
    "    \"sd2\"        : Model(\"sd2\", \"stabilityai/stable-diffusion-2-base\"),\n",
    "    \"sd-turbo\"   : Model(\"sd-turbo\", \"stabilityai/sd-turbo\"),\n",
    "    \"sdxl\"       : Model(\"sdxl\", \"stabilityai/stable-diffusion-xl-base-1.0\"),\n",
    "}\n",
    "\n",
    "model = models[\"sdxl\"].fp16()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed5b3f-2b52-46b4-9dc2-d7ae881e3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpu_monitor import GPUMonitor\n",
    "\n",
    "# The monitor is a handy way to see how the gpu handles different operations, but it is shown below the cell it runs in.  Thus\n",
    "# it can be helpful to use it around both the LLM loading as well as the handling of prompts.  Feel free to comment this out if\n",
    "# not needed.\n",
    "gpu_monitor = GPUMonitor()\n",
    "display(gpu_monitor.display());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002f935-e1f2-4bf3-8588-a0f372157569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "try:\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model.repo_id, local_files_only=True, **model.pretrain_options())\n",
    "except OSError as e:\n",
    "    print(f\"Model ({model.name}: {model.repo_id}) unavailable, downloading\")\n",
    "    pipe = AutoPipelineForText2Image.from_pretrained(model.repo_id, **model.pretrain_options())\n",
    "pipe.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628dd6e-664e-4995-a707-68b300b43aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The monitor is a handy way to see how the gpu handles different operations, but it is shown below the cell it runs in.  Thus\n",
    "# it can be helpful to use it around both the LLM loading as well as the handling of prompts.  Feel free to comment this out if\n",
    "# not needed.\n",
    "gpu_monitor = GPUMonitor()\n",
    "display(gpu_monitor.display());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccf32a-b3d8-4fac-a2ac-b8fa05f16c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cinematic shot of a sky whale flying through clouds at sunset.\"\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be62c53-8e11-4d87-87ce-3dfddc670019",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "<h4><span style=\"color:red\">Don't run until you are finished using the LLM.</span></h4>\n",
    "\n",
    "The following cell will reset the variables using memory on the GPU and clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426cdef-2018-4039-aed5-7db54ef610d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pipe=None\n",
    "torch.cuda.empty_cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824439ca-f020-4d5f-80cf-4a5bf18ecb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
