{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056c22ed-d110-46af-afa9-f16ce92e5189",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><span style=\"color:red\">This example that can be overwritten by container updates.</span></h3>\n",
    "<h4 style=\"text-align: center;\"><span style=\"color:red\">Please create a new notebook if you plan to make changes.</span></h4>\n",
    "\n",
    "## Intro\n",
    "\n",
    "This example jupyter file demonstrates basic chat functionality.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Run each cell to step through the process, including (first-time only) downloading a model to use.\n",
    "Note: Run cells, in order, at least once, to set up the environment.  Once set up, you can change the prompt and rerun just that cell.  If you make changes in a cell, that cell should be rerun for the changes to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b394ae-d112-4c9e-bf20-823ad1555f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import device_info\n",
    "\n",
    "device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eaaecd-124b-4a0b-ba17-d074888c2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    filename: str\n",
    "    repo_id: str\n",
    "\n",
    "models = {\n",
    "    \"codeninja\"   : Model(\"codeninja-1.0-openchat-7b.Q4_K_M.gguf\", \"TheBloke/CodeNinja-1.0-OpenChat-7B-GGUF\"),\n",
    "    \"mixtral\"     : Model(\"mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\", \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\"),\n",
    "}\n",
    "\n",
    "model = models[\"codeninja\"] # or change to \"mixtral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236ed2c-f183-454c-adc7-8bd8787e626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Uncommenting the following line _may_ improve download speeds\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Download the model if needed.\n",
    "hf_hub_download(repo_id=model.repo_id, filename=model.filename, local_dir=\"/app/data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4c5a7-470d-4fea-8e46-e6cdec3077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7ca21-a3bd-41a1-a1fa-9d85ea90b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0763da-12ad-46d5-bd47-4751432dd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693e3ef-449c-464f-bc98-578742f34508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpu_monitor import GPUMonitor\n",
    "\n",
    "# The monitor is a handy way to see how the gpu handles different operations, but it is shown below the cell it runs in.  Thus\n",
    "# it can be helpful to use it around both the LLM loading as well as the handling of prompts.  Feel free to comment this out if\n",
    "# not needed.\n",
    "gpu_monitor = GPUMonitor()\n",
    "display(gpu_monitor.display());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290788c-d024-47cc-bf92-eb831a5af72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 12  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=f\"/app/data/{model.filename}\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7bb3d-fe87-4b0c-b01f-a9baefa90a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The monitor is a handy way to see how the gpu handles different operations, but it is shown below the cell it runs in.  Thus\n",
    "# it can be helpful to use it around both the LLM loading as well as the handling of prompts.  Feel free to comment this out if\n",
    "# not needed.\n",
    "gpu_monitor = GPUMonitor()\n",
    "display(gpu_monitor.display());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e072d75-61fc-4d3e-8f37-8a244f051c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c8d5a-82c5-47cf-b7eb-9493156cab81",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "<h4><span style=\"color:red\">Don't run until you are finished using the LLM.</span></h4>\n",
    "\n",
    "The following cell will reset the variables using memory on the GPU and clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e318b9e-77e2-4bb2-9922-78913152b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "callback_manager=None\n",
    "llm_chain=None\n",
    "llm=None\n",
    "torch.cuda.empty_cache();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7535dd-f3f6-4d22-868d-63118e0689fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
